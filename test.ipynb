{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4eb982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5180, 20)\n",
      "Target distribution:\n",
      "Attrition\n",
      "0.0    0.721042\n",
      "1.0    0.278958\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "============================================================\n",
      "Training Stacking Ensemble with 5 Base Models\n",
      "============================================================\n",
      "\n",
      "Training on training set...\n",
      "\n",
      "============================================================\n",
      "Validation Accuracy: 0.984556\n",
      "============================================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       560\n",
      "         1.0       0.98      0.97      0.97       217\n",
      "\n",
      "    accuracy                           0.98       777\n",
      "   macro avg       0.98      0.98      0.98       777\n",
      "weighted avg       0.98      0.98      0.98       777\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[555   5]\n",
      " [  7 210]]\n",
      "\n",
      "============================================================\n",
      "Training on full dataset for final predictions...\n",
      "============================================================\n",
      "\n",
      "Performing 10-fold cross-validation on full data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation scores: [0.98648649 0.99034749 0.99227799 0.98455598 0.98262548 0.98841699\n",
      " 0.98841699 0.996139   0.99420849 0.98841699]\n",
      "Mean CV accuracy: 0.989189 (+/- 0.007950)\n",
      "Min CV accuracy: 0.982625\n",
      "Max CV accuracy: 0.996139\n",
      "\n",
      "============================================================\n",
      "Submission file created: submission_test.csv\n",
      "Total predictions: 2630\n",
      "Prediction distribution:\n",
      "0.0    1895\n",
      "1.0     735\n",
      "Name: count, dtype: int64\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(\"Train_Dataset.csv\")\n",
    "test_df = pd.read_csv(\"Test_Dataset.csv\")\n",
    "\n",
    "# separate features and target\n",
    "target_col = \"Attrition\"\n",
    "id_col = \"EmployeeID\"\n",
    "\n",
    "# drop rows where target is NaN\n",
    "train_df_clean = train_df[train_df[target_col].notna()].copy()\n",
    "\n",
    "X = train_df_clean.drop(columns=[target_col, id_col])\n",
    "y = train_df_clean[target_col]\n",
    "\n",
    "# identify column types\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts(normalize=True)}\")\n",
    "\n",
    "# Advanced preprocessing with feature engineering\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop='if_binary'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create multiple models for stacking\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=15,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "et_model = ExtraTreesClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=15,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_samples_split=4,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create stacking ensemble\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Stacking Ensemble with 5 Base Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "base_models = [\n",
    "    ('xgb', Pipeline([('preprocess', preprocessor), ('model', xgb_model)])),\n",
    "    ('lgbm', Pipeline([('preprocess', preprocessor), ('model', lgbm_model)])),\n",
    "    ('rf', Pipeline([('preprocess', preprocessor), ('model', rf_model)])),\n",
    "    ('et', Pipeline([('preprocess', preprocessor), ('model', et_model)])),\n",
    "    ('gb', Pipeline([('preprocess', preprocessor), ('model', gb_model)]))\n",
    "]\n",
    "\n",
    "# Meta-learner\n",
    "meta_learner = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train stacking model\n",
    "print(\"\\nTraining on training set...\")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = stacking_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.6f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_val))\n",
    "\n",
    "# Train on full dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training on full dataset for final predictions...\")\n",
    "print(\"=\"*60)\n",
    "stacking_model.fit(X, y)\n",
    "\n",
    "# Cross-validation on full data\n",
    "print(\"\\nPerforming 10-fold cross-validation on full data...\")\n",
    "cv_scores = cross_val_score(stacking_model, X, y, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.6f} (+/- {cv_scores.std() * 2:.6f})\")\n",
    "print(f\"Min CV accuracy: {cv_scores.min():.6f}\")\n",
    "print(f\"Max CV accuracy: {cv_scores.max():.6f}\")\n",
    "\n",
    "# predict on test set\n",
    "X_test = test_df.drop(columns=[id_col])\n",
    "test_preds = stacking_model.predict(X_test)\n",
    "\n",
    "# create submission file\n",
    "submission = pd.DataFrame({\n",
    "    id_col: test_df[id_col],\n",
    "    target_col: test_preds\n",
    "})\n",
    "submission.to_csv(\"submission_test.csv\", index=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Submission file created: submission_test.csv\")\n",
    "print(f\"Total predictions: {len(submission)}\")\n",
    "print(f\"Prediction distribution:\\n{pd.Series(test_preds).value_counts()}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
